{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Tile2Vec features for CDL classification\n",
    "In this notebook, we'll use a Tile2Vec model that has been pre-trained on the NAIP dataset to embed a small NAIP dataset and then train a classifier on the corresponding Cropland Data Layer (CDL) labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from time import time\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.tilenet import make_tilenet\n",
    "from src.resnet import ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Loading pre-trained model\n",
    "In this step, we will initialize a new TileNet model and then load the pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up model\n",
    "in_channels = 4\n",
    "z_dim = 512\n",
    "cuda = torch.cuda.is_available()\n",
    "tilenet = make_tilenet(in_channels=in_channels, z_dim=z_dim)\n",
    "#Use old model for now\n",
    "#tilenet = ResNet18()\n",
    "if cuda: tilenet.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TileNet(\n",
       "  (conv1): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer5): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load parameters\n",
    "model_fn = '../models/TileNet_epoch50-Copy1.ckpt'\n",
    "checkpoint = torch.load(model_fn)\n",
    "tilenet.load_state_dict(checkpoint)\n",
    "tilenet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Embed NAIP tiles\n",
    "In this step, we'll use TileNet to embed the NAIP tiles provided in `tile2vec/data/tiles`. There are 1000 tiles in total, named `1tile.npy` through `1000tile.npy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "tile_dir = '../data/tiles'\n",
    "n_tiles = 1000\n",
    "y = np.load(os.path.join(tile_dir, 'y.npy'))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded 1000 tiles: 8.370s\n"
     ]
    }
   ],
   "source": [
    "# Embed tiles\n",
    "t0 = time()\n",
    "X = np.zeros((n_tiles, z_dim))\n",
    "for idx in range(n_tiles):\n",
    "    tile = np.load(os.path.join(tile_dir, '{}tile.npy'.format(idx+1)))\n",
    "    # Get first 4 NAIP channels (5th is CDL mask)\n",
    "    tile = tile[:,:,:4]\n",
    "    # Rearrange to PyTorch order\n",
    "    tile = np.moveaxis(tile, -1, 0)\n",
    "    tile = np.expand_dims(tile, axis=0)\n",
    "    # Scale to [0, 1]\n",
    "    tile = tile / 255\n",
    "    # Embed tile\n",
    "    tile = torch.from_numpy(tile).float()\n",
    "    tile = Variable(tile)\n",
    "    if cuda: tile = tile.cuda()\n",
    "    z = tilenet.encode(tile)\n",
    "    if cuda: z = z.cpu()\n",
    "    z = z.data.numpy()\n",
    "    X[idx,:] = z\n",
    "t1 = time()\n",
    "print('Embedded {} tiles: {:0.3f}s'.format(n_tiles, t1-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Train random forest classifier\n",
    "In this step, we'll split the dataset into train and test sets and train a random forest classifier to predict CDL classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1.0, 2.0, 21.0, 24.0, 152.0, 28.0, 36.0, 176.0, 49.0, 54.0, 61.0, 69.0, 71.0, 72.0, 75.0, 76.0, 205.0, 204.0, 208.0, 212.0, 217.0, 225.0, 236.0, 111.0, 121.0, 122.0, 123.0, 124.0}\n"
     ]
    }
   ],
   "source": [
    "# Check CDL classes\n",
    "print(set(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the CDL classes are not numbered in consecutive order, we'll start by reindexing the classes from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27}\n"
     ]
    }
   ],
   "source": [
    "# Reindex CDL classes\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "print(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=512):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.bn = nn.BatchNorm1d(256) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 28)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MLP(model, data_loader, optimizer, lr, epoch):\n",
    "    model.train()\n",
    "    cuda = torch.cuda.is_available()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for data, label in data_loader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "            label = label.cuda()\n",
    "        \n",
    "        criterion = torch.nn.BCELoss()\n",
    "        y_pred = model(data)\n",
    "        loss = criterion(y_pred, label)\n",
    "        \n",
    "        print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can randomly split the data and train a random forest classifier many times to get an estimate of the average accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-7330021035d7>:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x = self.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.16242678463459015\n",
      "Epoch 0: train loss: 0.14570342004299164\n",
      "Epoch 0: train loss: 0.13016077876091003\n",
      "Epoch 0: train loss: 0.12146203964948654\n",
      "Epoch 1: train loss: 0.10028119385242462\n",
      "Epoch 1: train loss: 0.09376434236764908\n",
      "Epoch 1: train loss: 0.08974016457796097\n",
      "Epoch 1: train loss: 0.09337861835956573\n",
      "Epoch 2: train loss: 0.07610122859477997\n",
      "Epoch 2: train loss: 0.07242891192436218\n",
      "Epoch 2: train loss: 0.07320427149534225\n",
      "Epoch 2: train loss: 0.0832841694355011\n",
      "Epoch 3: train loss: 0.06660959124565125\n",
      "Epoch 3: train loss: 0.06341216713190079\n",
      "Epoch 3: train loss: 0.06483393162488937\n",
      "Epoch 3: train loss: 0.07890687137842178\n",
      "Epoch 4: train loss: 0.0609816238284111\n",
      "Epoch 4: train loss: 0.058468159288167953\n",
      "Epoch 4: train loss: 0.059274282306432724\n",
      "Epoch 4: train loss: 0.0759292021393776\n",
      "Epoch 5: train loss: 0.05660459026694298\n",
      "Epoch 5: train loss: 0.0552455298602581\n",
      "Epoch 5: train loss: 0.055543091148138046\n",
      "Epoch 5: train loss: 0.07376072555780411\n",
      "Epoch 6: train loss: 0.05296553298830986\n",
      "Epoch 6: train loss: 0.05273515358567238\n",
      "Epoch 6: train loss: 0.052705392241477966\n",
      "Epoch 6: train loss: 0.07181643694639206\n",
      "Epoch 7: train loss: 0.049731358885765076\n",
      "Epoch 7: train loss: 0.05030382424592972\n",
      "Epoch 7: train loss: 0.05035138875246048\n",
      "Epoch 7: train loss: 0.07022296637296677\n",
      "Epoch 8: train loss: 0.047005537897348404\n",
      "Epoch 8: train loss: 0.04813680425286293\n",
      "Epoch 8: train loss: 0.04850830137729645\n",
      "Epoch 8: train loss: 0.06890261918306351\n",
      "Epoch 9: train loss: 0.04451839253306389\n",
      "Epoch 9: train loss: 0.04608156904578209\n",
      "Epoch 9: train loss: 0.04647138714790344\n",
      "Epoch 9: train loss: 0.06736092269420624\n",
      "Epoch 0: train loss: 0.1565679907798767\n",
      "Epoch 0: train loss: 0.13806144893169403\n",
      "Epoch 0: train loss: 0.1237565353512764\n",
      "Epoch 0: train loss: 0.11044307798147202\n",
      "Epoch 1: train loss: 0.09470991045236588\n",
      "Epoch 1: train loss: 0.08838999271392822\n",
      "Epoch 1: train loss: 0.08622866123914719\n",
      "Epoch 1: train loss: 0.08021018654108047\n",
      "Epoch 2: train loss: 0.07223039865493774\n",
      "Epoch 2: train loss: 0.07073065638542175\n",
      "Epoch 2: train loss: 0.07322579622268677\n",
      "Epoch 2: train loss: 0.06908480823040009\n",
      "Epoch 3: train loss: 0.0632437989115715\n",
      "Epoch 3: train loss: 0.06315698474645615\n",
      "Epoch 3: train loss: 0.06728700548410416\n",
      "Epoch 3: train loss: 0.06429628282785416\n",
      "Epoch 4: train loss: 0.05786503478884697\n",
      "Epoch 4: train loss: 0.05912382900714874\n",
      "Epoch 4: train loss: 0.06395988166332245\n",
      "Epoch 4: train loss: 0.06194695085287094\n",
      "Epoch 5: train loss: 0.0534718856215477\n",
      "Epoch 5: train loss: 0.05591220408678055\n",
      "Epoch 5: train loss: 0.06132693588733673\n",
      "Epoch 5: train loss: 0.060409195721149445\n",
      "Epoch 6: train loss: 0.04956967383623123\n",
      "Epoch 6: train loss: 0.052715856581926346\n",
      "Epoch 6: train loss: 0.05898278206586838\n",
      "Epoch 6: train loss: 0.05921822041273117\n",
      "Epoch 7: train loss: 0.04660859704017639\n",
      "Epoch 7: train loss: 0.04999249055981636\n",
      "Epoch 7: train loss: 0.05696512386202812\n",
      "Epoch 7: train loss: 0.05741699039936066\n",
      "Epoch 8: train loss: 0.044035498052835464\n",
      "Epoch 8: train loss: 0.047878362238407135\n",
      "Epoch 8: train loss: 0.054989274591207504\n",
      "Epoch 8: train loss: 0.05516321212053299\n",
      "Epoch 9: train loss: 0.04136989638209343\n",
      "Epoch 9: train loss: 0.04634275287389755\n",
      "Epoch 9: train loss: 0.05313419923186302\n",
      "Epoch 9: train loss: 0.05346772447228432\n",
      "Epoch 0: train loss: 0.15804459154605865\n",
      "Epoch 0: train loss: 0.14141449332237244\n",
      "Epoch 0: train loss: 0.1262005865573883\n",
      "Epoch 0: train loss: 0.11526788026094437\n",
      "Epoch 1: train loss: 0.09382347762584686\n",
      "Epoch 1: train loss: 0.08913718909025192\n",
      "Epoch 1: train loss: 0.08725642412900925\n",
      "Epoch 1: train loss: 0.0870203748345375\n",
      "Epoch 2: train loss: 0.0692204087972641\n",
      "Epoch 2: train loss: 0.07160165905952454\n",
      "Epoch 2: train loss: 0.07251758128404617\n",
      "Epoch 2: train loss: 0.07869640737771988\n",
      "Epoch 3: train loss: 0.05950777232646942\n",
      "Epoch 3: train loss: 0.06490404903888702\n",
      "Epoch 3: train loss: 0.06512797623872757\n",
      "Epoch 3: train loss: 0.07538297027349472\n",
      "Epoch 4: train loss: 0.05380665138363838\n",
      "Epoch 4: train loss: 0.06002507731318474\n",
      "Epoch 4: train loss: 0.06053485348820686\n",
      "Epoch 4: train loss: 0.0732065737247467\n",
      "Epoch 5: train loss: 0.04984595626592636\n",
      "Epoch 5: train loss: 0.055796731263399124\n",
      "Epoch 5: train loss: 0.05746740102767944\n",
      "Epoch 5: train loss: 0.07142621278762817\n",
      "Epoch 6: train loss: 0.04647305607795715\n",
      "Epoch 6: train loss: 0.05219593644142151\n",
      "Epoch 6: train loss: 0.055586960166692734\n",
      "Epoch 6: train loss: 0.07025023549795151\n",
      "Epoch 7: train loss: 0.04348759353160858\n",
      "Epoch 7: train loss: 0.049353551119565964\n",
      "Epoch 7: train loss: 0.05375346541404724\n",
      "Epoch 7: train loss: 0.06917233020067215\n",
      "Epoch 8: train loss: 0.04087178409099579\n",
      "Epoch 8: train loss: 0.04682789370417595\n",
      "Epoch 8: train loss: 0.05141717195510864\n",
      "Epoch 8: train loss: 0.06797430664300919\n",
      "Epoch 9: train loss: 0.03892407566308975\n",
      "Epoch 9: train loss: 0.04451344534754753\n",
      "Epoch 9: train loss: 0.04951348900794983\n",
      "Epoch 9: train loss: 0.0665072426199913\n",
      "Epoch 0: train loss: 0.15812307596206665\n",
      "Epoch 0: train loss: 0.14324872195720673\n",
      "Epoch 0: train loss: 0.1276930719614029\n",
      "Epoch 0: train loss: 0.1094912588596344\n",
      "Epoch 1: train loss: 0.09968017786741257\n",
      "Epoch 1: train loss: 0.09872704744338989\n",
      "Epoch 1: train loss: 0.08893150091171265\n",
      "Epoch 1: train loss: 0.07739000767469406\n",
      "Epoch 2: train loss: 0.07795052230358124\n",
      "Epoch 2: train loss: 0.08159150183200836\n",
      "Epoch 2: train loss: 0.0737224891781807\n",
      "Epoch 2: train loss: 0.06501846015453339\n",
      "Epoch 3: train loss: 0.06845716387033463\n",
      "Epoch 3: train loss: 0.07472781836986542\n",
      "Epoch 3: train loss: 0.06705690920352936\n",
      "Epoch 3: train loss: 0.059379905462265015\n",
      "Epoch 4: train loss: 0.06117410957813263\n",
      "Epoch 4: train loss: 0.0702974945306778\n",
      "Epoch 4: train loss: 0.06272868812084198\n",
      "Epoch 4: train loss: 0.056317321956157684\n",
      "Epoch 5: train loss: 0.055644769221544266\n",
      "Epoch 5: train loss: 0.0667107030749321\n",
      "Epoch 5: train loss: 0.059764452278614044\n",
      "Epoch 5: train loss: 0.05425027012825012\n",
      "Epoch 6: train loss: 0.05166538804769516\n",
      "Epoch 6: train loss: 0.06299936771392822\n",
      "Epoch 6: train loss: 0.05689031258225441\n",
      "Epoch 6: train loss: 0.052184659987688065\n",
      "Epoch 7: train loss: 0.04870576411485672\n",
      "Epoch 7: train loss: 0.059987034648656845\n",
      "Epoch 7: train loss: 0.054785143584012985\n",
      "Epoch 7: train loss: 0.05073724314570427\n",
      "Epoch 8: train loss: 0.045958057045936584\n",
      "Epoch 8: train loss: 0.057421132922172546\n",
      "Epoch 8: train loss: 0.05286674574017525\n",
      "Epoch 8: train loss: 0.049488164484500885\n",
      "Epoch 9: train loss: 0.04297391697764397\n",
      "Epoch 9: train loss: 0.055179446935653687\n",
      "Epoch 9: train loss: 0.05137341096997261\n",
      "Epoch 9: train loss: 0.04870850592851639\n",
      "Epoch 0: train loss: 0.1595829278230667\n",
      "Epoch 0: train loss: 0.14159882068634033\n",
      "Epoch 0: train loss: 0.12621811032295227\n",
      "Epoch 0: train loss: 0.11527779698371887\n",
      "Epoch 1: train loss: 0.09835241734981537\n",
      "Epoch 1: train loss: 0.09164382517337799\n",
      "Epoch 1: train loss: 0.0869324579834938\n",
      "Epoch 1: train loss: 0.08722945302724838\n",
      "Epoch 2: train loss: 0.07545628398656845\n",
      "Epoch 2: train loss: 0.07157471776008606\n",
      "Epoch 2: train loss: 0.07379651814699173\n",
      "Epoch 2: train loss: 0.08008578419685364\n",
      "Epoch 3: train loss: 0.06618756055831909\n",
      "Epoch 3: train loss: 0.06244434788823128\n",
      "Epoch 3: train loss: 0.06659861654043198\n",
      "Epoch 3: train loss: 0.0772852897644043\n",
      "Epoch 4: train loss: 0.060557153075933456\n",
      "Epoch 4: train loss: 0.0568634569644928\n",
      "Epoch 4: train loss: 0.06122012436389923\n",
      "Epoch 4: train loss: 0.07498784363269806\n",
      "Epoch 5: train loss: 0.056931834667921066\n",
      "Epoch 5: train loss: 0.05307242274284363\n",
      "Epoch 5: train loss: 0.05768989026546478\n",
      "Epoch 5: train loss: 0.07302861660718918\n",
      "Epoch 6: train loss: 0.05397429317235947\n",
      "Epoch 6: train loss: 0.04973084479570389\n",
      "Epoch 6: train loss: 0.05526687949895859\n",
      "Epoch 6: train loss: 0.07128532230854034\n",
      "Epoch 7: train loss: 0.050956811755895615\n",
      "Epoch 7: train loss: 0.04645015299320221\n",
      "Epoch 7: train loss: 0.05338216945528984\n",
      "Epoch 7: train loss: 0.07016810029745102\n",
      "Epoch 8: train loss: 0.04815595969557762\n",
      "Epoch 8: train loss: 0.043937887996435165\n",
      "Epoch 8: train loss: 0.05178157240152359\n",
      "Epoch 8: train loss: 0.06983406841754913\n",
      "Epoch 9: train loss: 0.04550699517130852\n",
      "Epoch 9: train loss: 0.04216362163424492\n",
      "Epoch 9: train loss: 0.0496235117316246\n",
      "Epoch 9: train loss: 0.06911980360746384\n",
      "Epoch 0: train loss: 0.15977947413921356\n",
      "Epoch 0: train loss: 0.14195510745048523\n",
      "Epoch 0: train loss: 0.1273050606250763\n",
      "Epoch 0: train loss: 0.11795348674058914\n",
      "Epoch 1: train loss: 0.09265448153018951\n",
      "Epoch 1: train loss: 0.09290342032909393\n",
      "Epoch 1: train loss: 0.086507149040699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss: 0.0894862711429596\n",
      "Epoch 2: train loss: 0.0664818212389946\n",
      "Epoch 2: train loss: 0.07498897612094879\n",
      "Epoch 2: train loss: 0.07221861928701401\n",
      "Epoch 2: train loss: 0.08045878261327744\n",
      "Epoch 3: train loss: 0.05751475691795349\n",
      "Epoch 3: train loss: 0.06791173666715622\n",
      "Epoch 3: train loss: 0.06629333645105362\n",
      "Epoch 3: train loss: 0.07680609822273254\n",
      "Epoch 4: train loss: 0.053164541721343994\n",
      "Epoch 4: train loss: 0.06361541897058487\n",
      "Epoch 4: train loss: 0.06190267950296402\n",
      "Epoch 4: train loss: 0.07409409433603287\n",
      "Epoch 5: train loss: 0.049796294420957565\n",
      "Epoch 5: train loss: 0.05998372659087181\n",
      "Epoch 5: train loss: 0.058079253882169724\n",
      "Epoch 5: train loss: 0.07173601537942886\n",
      "Epoch 6: train loss: 0.04717143997550011\n",
      "Epoch 6: train loss: 0.05693846568465233\n",
      "Epoch 6: train loss: 0.054733239114284515\n",
      "Epoch 6: train loss: 0.0696667805314064\n",
      "Epoch 7: train loss: 0.04468126967549324\n",
      "Epoch 7: train loss: 0.054020095616579056\n",
      "Epoch 7: train loss: 0.0519133098423481\n",
      "Epoch 7: train loss: 0.06808003783226013\n",
      "Epoch 8: train loss: 0.042301151901483536\n",
      "Epoch 8: train loss: 0.051274172961711884\n",
      "Epoch 8: train loss: 0.04991165176033974\n",
      "Epoch 8: train loss: 0.06666285544633865\n",
      "Epoch 9: train loss: 0.04019009321928024\n",
      "Epoch 9: train loss: 0.04873816296458244\n",
      "Epoch 9: train loss: 0.048099614679813385\n",
      "Epoch 9: train loss: 0.06509532034397125\n",
      "Epoch 0: train loss: 0.15620385110378265\n",
      "Epoch 0: train loss: 0.13567499816417694\n",
      "Epoch 0: train loss: 0.12356100976467133\n",
      "Epoch 0: train loss: 0.1099686399102211\n",
      "Epoch 1: train loss: 0.09704966098070145\n",
      "Epoch 1: train loss: 0.08753074705600739\n",
      "Epoch 1: train loss: 0.08965633064508438\n",
      "Epoch 1: train loss: 0.08131632208824158\n",
      "Epoch 2: train loss: 0.07587670534849167\n",
      "Epoch 2: train loss: 0.07005215436220169\n",
      "Epoch 2: train loss: 0.07776354998350143\n",
      "Epoch 2: train loss: 0.06986738741397858\n",
      "Epoch 3: train loss: 0.06792700290679932\n",
      "Epoch 3: train loss: 0.06289376318454742\n",
      "Epoch 3: train loss: 0.07256333529949188\n",
      "Epoch 3: train loss: 0.06413042545318604\n",
      "Epoch 4: train loss: 0.06268436461687088\n",
      "Epoch 4: train loss: 0.05858934670686722\n",
      "Epoch 4: train loss: 0.06850304454565048\n",
      "Epoch 4: train loss: 0.06022823229432106\n",
      "Epoch 5: train loss: 0.0581042654812336\n",
      "Epoch 5: train loss: 0.05574954301118851\n",
      "Epoch 5: train loss: 0.0653609111905098\n",
      "Epoch 5: train loss: 0.05782054737210274\n",
      "Epoch 6: train loss: 0.05438276007771492\n",
      "Epoch 6: train loss: 0.05329267308115959\n",
      "Epoch 6: train loss: 0.06261126697063446\n",
      "Epoch 6: train loss: 0.05565755069255829\n",
      "Epoch 7: train loss: 0.05105585232377052\n",
      "Epoch 7: train loss: 0.05055743455886841\n",
      "Epoch 7: train loss: 0.06053277105093002\n",
      "Epoch 7: train loss: 0.05380576103925705\n",
      "Epoch 8: train loss: 0.048296231776475906\n",
      "Epoch 8: train loss: 0.04795397073030472\n",
      "Epoch 8: train loss: 0.05890486389398575\n",
      "Epoch 8: train loss: 0.05253298208117485\n",
      "Epoch 9: train loss: 0.045629605650901794\n",
      "Epoch 9: train loss: 0.045542456209659576\n",
      "Epoch 9: train loss: 0.05738363042473793\n",
      "Epoch 9: train loss: 0.05114414542913437\n",
      "Epoch 0: train loss: 0.15935978293418884\n",
      "Epoch 0: train loss: 0.14163699746131897\n",
      "Epoch 0: train loss: 0.13002632558345795\n",
      "Epoch 0: train loss: 0.11566925048828125\n",
      "Epoch 1: train loss: 0.09574878960847855\n",
      "Epoch 1: train loss: 0.08904242515563965\n",
      "Epoch 1: train loss: 0.09541219472885132\n",
      "Epoch 1: train loss: 0.08518049120903015\n",
      "Epoch 2: train loss: 0.07148473709821701\n",
      "Epoch 2: train loss: 0.06760718673467636\n",
      "Epoch 2: train loss: 0.0846162736415863\n",
      "Epoch 2: train loss: 0.07260551303625107\n",
      "Epoch 3: train loss: 0.0624227337539196\n",
      "Epoch 3: train loss: 0.058673515915870667\n",
      "Epoch 3: train loss: 0.08096155524253845\n",
      "Epoch 3: train loss: 0.06591722369194031\n",
      "Epoch 4: train loss: 0.05729604884982109\n",
      "Epoch 4: train loss: 0.05344425514340401\n",
      "Epoch 4: train loss: 0.07833816111087799\n",
      "Epoch 4: train loss: 0.06164921447634697\n",
      "Epoch 5: train loss: 0.05314790830016136\n",
      "Epoch 5: train loss: 0.04962119832634926\n",
      "Epoch 5: train loss: 0.07576216757297516\n",
      "Epoch 5: train loss: 0.0588047057390213\n",
      "Epoch 6: train loss: 0.049398139119148254\n",
      "Epoch 6: train loss: 0.04670967161655426\n",
      "Epoch 6: train loss: 0.0737258568406105\n",
      "Epoch 6: train loss: 0.056813791394233704\n",
      "Epoch 7: train loss: 0.046434152871370316\n",
      "Epoch 7: train loss: 0.04437374323606491\n",
      "Epoch 7: train loss: 0.07180624455213547\n",
      "Epoch 7: train loss: 0.05500272288918495\n",
      "Epoch 8: train loss: 0.04412828013300896\n",
      "Epoch 8: train loss: 0.04204467311501503\n",
      "Epoch 8: train loss: 0.06952952593564987\n",
      "Epoch 8: train loss: 0.05324658006429672\n",
      "Epoch 9: train loss: 0.04217851907014847\n",
      "Epoch 9: train loss: 0.03986165300011635\n",
      "Epoch 9: train loss: 0.06744614988565445\n",
      "Epoch 9: train loss: 0.052196577191352844\n",
      "Epoch 0: train loss: 0.16391108930110931\n",
      "Epoch 0: train loss: 0.14692506194114685\n",
      "Epoch 0: train loss: 0.12767373025417328\n",
      "Epoch 0: train loss: 0.1209350973367691\n",
      "Epoch 1: train loss: 0.10062684863805771\n",
      "Epoch 1: train loss: 0.09535744786262512\n",
      "Epoch 1: train loss: 0.08277997374534607\n",
      "Epoch 1: train loss: 0.09227340668439865\n",
      "Epoch 2: train loss: 0.07478318363428116\n",
      "Epoch 2: train loss: 0.07614514976739883\n",
      "Epoch 2: train loss: 0.06779222190380096\n",
      "Epoch 2: train loss: 0.08200368285179138\n",
      "Epoch 3: train loss: 0.0644313171505928\n",
      "Epoch 3: train loss: 0.06860815733671188\n",
      "Epoch 3: train loss: 0.061807937920093536\n",
      "Epoch 3: train loss: 0.07704998552799225\n",
      "Epoch 4: train loss: 0.058474861085414886\n",
      "Epoch 4: train loss: 0.06399441510438919\n",
      "Epoch 4: train loss: 0.057435132563114166\n",
      "Epoch 4: train loss: 0.07351251691579819\n",
      "Epoch 5: train loss: 0.053942348808050156\n",
      "Epoch 5: train loss: 0.06041593849658966\n",
      "Epoch 5: train loss: 0.0538664348423481\n",
      "Epoch 5: train loss: 0.07080379128456116\n",
      "Epoch 6: train loss: 0.050332337617874146\n",
      "Epoch 6: train loss: 0.057547543197870255\n",
      "Epoch 6: train loss: 0.05105281248688698\n",
      "Epoch 6: train loss: 0.06878379732370377\n",
      "Epoch 7: train loss: 0.04731445387005806\n",
      "Epoch 7: train loss: 0.055050015449523926\n",
      "Epoch 7: train loss: 0.0491277314722538\n",
      "Epoch 7: train loss: 0.06715639680624008\n",
      "Epoch 8: train loss: 0.04454615339636803\n",
      "Epoch 8: train loss: 0.05272629112005234\n",
      "Epoch 8: train loss: 0.04747762531042099\n",
      "Epoch 8: train loss: 0.0653490424156189\n",
      "Epoch 9: train loss: 0.041929762810468674\n",
      "Epoch 9: train loss: 0.050655435770750046\n",
      "Epoch 9: train loss: 0.045747578144073486\n",
      "Epoch 9: train loss: 0.06419368833303452\n",
      "Epoch 0: train loss: 0.15514053404331207\n",
      "Epoch 0: train loss: 0.14178764820098877\n",
      "Epoch 0: train loss: 0.12640175223350525\n",
      "Epoch 0: train loss: 0.11405114084482193\n",
      "Epoch 1: train loss: 0.09352820366621017\n",
      "Epoch 1: train loss: 0.0954706221818924\n",
      "Epoch 1: train loss: 0.0915166586637497\n",
      "Epoch 1: train loss: 0.08603981137275696\n",
      "Epoch 2: train loss: 0.070154570043087\n",
      "Epoch 2: train loss: 0.07621747255325317\n",
      "Epoch 2: train loss: 0.0804121270775795\n",
      "Epoch 2: train loss: 0.0754418596625328\n",
      "Epoch 3: train loss: 0.06209976226091385\n",
      "Epoch 3: train loss: 0.06680361926555634\n",
      "Epoch 3: train loss: 0.07561664283275604\n",
      "Epoch 3: train loss: 0.06889168918132782\n",
      "Epoch 4: train loss: 0.05687132477760315\n",
      "Epoch 4: train loss: 0.06067344546318054\n",
      "Epoch 4: train loss: 0.0724710077047348\n",
      "Epoch 4: train loss: 0.06472348421812057\n",
      "Epoch 5: train loss: 0.05310029163956642\n",
      "Epoch 5: train loss: 0.05659770965576172\n",
      "Epoch 5: train loss: 0.06966143101453781\n",
      "Epoch 5: train loss: 0.06254726648330688\n",
      "Epoch 6: train loss: 0.049934811890125275\n",
      "Epoch 6: train loss: 0.05312439426779747\n",
      "Epoch 6: train loss: 0.06595835834741592\n",
      "Epoch 6: train loss: 0.061622440814971924\n",
      "Epoch 7: train loss: 0.04719875752925873\n",
      "Epoch 7: train loss: 0.05020926147699356\n",
      "Epoch 7: train loss: 0.062492381781339645\n",
      "Epoch 7: train loss: 0.06108925864100456\n",
      "Epoch 8: train loss: 0.04502243921160698\n",
      "Epoch 8: train loss: 0.047787997871637344\n",
      "Epoch 8: train loss: 0.059962011873722076\n",
      "Epoch 8: train loss: 0.059681713581085205\n",
      "Epoch 9: train loss: 0.0427984744310379\n",
      "Epoch 9: train loss: 0.045853979885578156\n",
      "Epoch 9: train loss: 0.058060768991708755\n",
      "Epoch 9: train loss: 0.05730479583144188\n",
      "Mean accuracy: 66.1000\n",
      "Standard deviation: 2.2338\n"
     ]
    }
   ],
   "source": [
    "n_trials = 10\n",
    "accs = np.zeros((n_trials,))\n",
    "\n",
    "lr = 1e-3\n",
    "epochs = 10\n",
    "\n",
    "for i in range(n_trials):\n",
    "    model = MLP()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    cuda = torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        model = model.cuda()\n",
    "    \n",
    "    # Splitting data and training RF classifer\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2)    \n",
    "    y_tr = np.eye(28)[y_tr]\n",
    "    \n",
    "    X_tr = torch.Tensor(X_tr)\n",
    "    X_te = torch.Tensor(X_te)\n",
    "    y_tr = torch.Tensor(y_tr)\n",
    "    y_te = torch.Tensor(y_te)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_tr, y_tr)\n",
    "    test_dataset = TensorDataset(X_te, y_te)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = 200)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=200)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        train_MLP(model, train_dataloader, optimizer, lr, e)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for data, label in test_dataloader:\n",
    "        if cuda:\n",
    "            data = data.cuda()\n",
    "            label = label.cuda()\n",
    "        \n",
    "        output_test = model(data)\n",
    "        _, predicted = torch.max(output_test, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "    \n",
    "    accuracy = correct * 100 / total    \n",
    "    accs[i] = accuracy\n",
    "    \n",
    "print('Mean accuracy: {:0.4f}'.format(accs.mean()))\n",
    "print('Standard deviation: {:0.4f}'.format(accs.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
